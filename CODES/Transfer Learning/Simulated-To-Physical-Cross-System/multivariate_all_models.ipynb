{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "34hh7YYS0G00",
    "outputId": "2af10035-5e7f-4591-b730-8bb3f331c1d0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVsYosjT0G1A"
   },
   "outputs": [],
   "source": [
    "def encode_text_features(encode_decode, data_frame, encoder_isa=None, encoder_mem_type=None):\n",
    "    # Implement Categorical OneHot encoding for ISA and mem-type\n",
    "    if encode_decode == 'encode':\n",
    "        encoder_isa = ce.one_hot.OneHotEncoder(cols=['isa'])\n",
    "        encoder_mem_type = ce.one_hot.OneHotEncoder(cols=['mem-type'])\n",
    "        encoder_isa.fit(data_frame, verbose=1)\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        encoder_mem_type.fit(df_new1, verbose=1)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "    else:\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "        \n",
    "    return encoded_data_frame, encoder_isa, encoder_mem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwxxEzWA0G1G"
   },
   "outputs": [],
   "source": [
    "def absolute_percentage_error(Y_test, Y_pred):\n",
    "    error = 0\n",
    "    for i in range(len(Y_test)):\n",
    "        if(Y_test[i]!= 0 ):\n",
    "            error = error + (abs(Y_test[i] - Y_pred[i]))/Y_test[i]\n",
    "        \n",
    "    error = error/ len(Y_test)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QOjg3qU1eo8"
   },
   "outputs": [],
   "source": [
    "def return_best_param(model, grid, X_train, Y_train):\n",
    "    grid = GridSearchCV(model, grid, refit = True, verbose = 0)\n",
    "    # fitting the model for grid search \n",
    "    tqdm(grid.fit(X_train, Y_train)) \n",
    "    print('Found Best Parameters for this model', model)\n",
    "\n",
    "    # print how our model looks after hyper-parameter tuning \n",
    "    return (grid.best_estimator_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Edkj6tmx0G1M"
   },
   "source": [
    "# Dataset 1 :Qsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npQlrKff0G1O"
   },
   "outputs": [],
   "source": [
    "def process_all_qsort(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','isa_1', 'bus_speed','num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_qsort' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eV5noIsE0G1U",
    "outputId": "ae8e09a0-e954-4f79-b279-48460b5f7c85",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') 19 19\n",
      "(2730, 17) (672, 17) (2730, 2) (672, 2)\n",
      "(67, 17) (605, 17) (67, 2) (605, 2)\n",
      "(2797, 17) (2797, 2) (605, 17) (605, 2)\n",
      "(2797, 17) (605, 17) (2797, 2) (605, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(2797, 17) (605, 17) (2797, 2) (605, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(2797, 17) (605, 17) (2797, 2) (605, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(2797, 17) (605, 17) (2797, 2) (605, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(2797, 17) (605, 17) (2797, 2) (605, 2)\n",
      "  model_name     dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  qsort_simulated  0.202976      0.388767    1.007209\n",
      "1    best_dt  qsort_simulated -0.216541      0.433883    0.940607\n",
      "2    best_rf  qsort_simulated -0.163538      0.380392    0.951069\n",
      "3   best_etr  qsort_simulated  0.269197      0.316673    0.817784\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'qsort_physical'\n",
    "dataset_name = 'qsort_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_qsort(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dijkstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4',\n",
    "                                                        'isa_1','isa_2' ,'isa_3', 'isa_4', 'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_dijkstra' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') 18 18\n",
      "(362, 16) (52, 16) (362, 2) (52, 2)\n",
      "(5, 16) (47, 16) (5, 2) (47, 2)\n",
      "(367, 16) (367, 2) (47, 16) (47, 2)\n",
      "(367, 16) (47, 16) (367, 2) (47, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(367, 16) (47, 16) (367, 2) (47, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(367, 16) (47, 16) (367, 2) (47, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(367, 16) (47, 16) (367, 2) (47, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(367, 16) (47, 16) (367, 2) (47, 2)\n",
      "  model_name        dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  dijkstra_simulated  0.026137      0.149727    0.568331\n",
      "1    best_dt  dijkstra_simulated  0.392961      0.178941    0.152443\n",
      "2    best_rf  dijkstra_simulated -0.055618      0.165334    0.314762\n",
      "3   best_etr  dijkstra_simulated  0.317965      0.134089    0.404830\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'dijkstra_physical'\n",
    "dataset_name = 'dijkstra_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_dijkstra(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_matmul(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','isa_1',\n",
    "                                                        'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_matmul' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') 19 19\n",
      "(1780, 17) (519, 17) (1780, 2) (519, 2)\n",
      "(51, 17) (468, 17) (51, 2) (468, 2)\n",
      "(1831, 17) (1831, 2) (468, 17) (468, 2)\n",
      "(1831, 17) (468, 17) (1831, 2) (468, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(1831, 17) (468, 17) (1831, 2) (468, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(1831, 17) (468, 17) (1831, 2) (468, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(1831, 17) (468, 17) (1831, 2) (468, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(1831, 17) (468, 17) (1831, 2) (468, 2)\n",
      "  model_name      dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  matmul_simulated  0.043162      1.710582    0.882659\n",
      "1    best_dt  matmul_simulated -1.375098      1.866858    1.061296\n",
      "2    best_rf  matmul_simulated -0.142341      1.792911    0.720910\n",
      "3   best_etr  matmul_simulated  0.213307      1.721895    0.658918\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'matmul_physical'\n",
    "dataset_name = 'matmul_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_matmul(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_tracking(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4',\n",
    "                                                        'isa_1','isa_2' ,'isa_3', 'isa_4', 'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_tracking' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') 18 18\n",
      "(425, 16) (52, 16) (425, 2) (52, 2)\n",
      "(5, 16) (47, 16) (5, 2) (47, 2)\n",
      "(430, 16) (430, 2) (47, 16) (47, 2)\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "  model_name        dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  tracking_simulated -0.069951      0.161222    0.621600\n",
      "1    best_dt  tracking_simulated  0.278447      0.154937    0.530743\n",
      "2    best_rf  tracking_simulated  0.219040      0.141020    0.515168\n",
      "3   best_etr  tracking_simulated  0.202932      0.136964    0.521021\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'tracking_physical'\n",
    "dataset_name = 'tracking_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_tracking(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_svm(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4',\n",
    "                                                        'isa_1','isa_2' ,'isa_3', 'isa_4', 'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_svm' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') 18 18\n",
      "(390, 16) (52, 16) (390, 2) (52, 2)\n",
      "(5, 16) (47, 16) (5, 2) (47, 2)\n",
      "(395, 16) (395, 2) (47, 16) (47, 2)\n",
      "(395, 16) (47, 16) (395, 2) (47, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(395, 16) (47, 16) (395, 2) (47, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(395, 16) (47, 16) (395, 2) (47, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(395, 16) (47, 16) (395, 2) (47, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(395, 16) (47, 16) (395, 2) (47, 2)\n",
      "  model_name   dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  svm_simulated -0.522287      0.111705    0.579131\n",
      "1    best_dt  svm_simulated  0.208404      0.113978    0.378004\n",
      "2    best_rf  svm_simulated  0.097215      0.109155    0.426517\n",
      "3   best_etr  svm_simulated  0.029332      0.107286    0.434542\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'svm_physical'\n",
    "dataset_name = 'svm_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_svm(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montecarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_montecarlo(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','isa_1',\n",
    "                                                        'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_montecarlo' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') 19 19\n",
      "(1365, 17) (260, 17) (1365, 2) (260, 2)\n",
      "(26, 17) (234, 17) (26, 2) (234, 2)\n",
      "(1391, 17) (1391, 2) (234, 17) (234, 2)\n",
      "(1391, 17) (234, 17) (1391, 2) (234, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(1391, 17) (234, 17) (1391, 2) (234, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(1391, 17) (234, 17) (1391, 2) (234, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(1391, 17) (234, 17) (1391, 2) (234, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(1391, 17) (234, 17) (1391, 2) (234, 2)\n",
      "  model_name                dataset_name         r2  mape_runtime  mape_power\n",
      "0   best_knn  montecarlocalcpi_simulated   0.010851      2.475787    0.743036\n",
      "1    best_dt  montecarlocalcpi_simulated  -0.186620      1.919412    0.656083\n",
      "2    best_rf  montecarlocalcpi_simulated  -0.073915      2.934440    0.617910\n",
      "3   best_etr  montecarlocalcpi_simulated -11.288312      6.021901    0.589704\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'montecarlocalcpi_physical'\n",
    "dataset_name = 'montecarlocalcpi_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_montecarlo(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_mser(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4',\n",
    "                                                        'isa_1','isa_2' ,'isa_3', 'isa_4', 'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_mser' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') 18 18\n",
      "(430, 16) (52, 16) (430, 2) (52, 2)\n",
      "(5, 16) (47, 16) (5, 2) (47, 2)\n",
      "(435, 16) (435, 2) (47, 16) (47, 2)\n",
      "(435, 16) (47, 16) (435, 2) (47, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(435, 16) (47, 16) (435, 2) (47, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(435, 16) (47, 16) (435, 2) (47, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(435, 16) (47, 16) (435, 2) (47, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(435, 16) (47, 16) (435, 2) (47, 2)\n",
      "  model_name    dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  mser_simulated -0.127201      0.278283    0.717318\n",
      "1    best_dt  mser_simulated -2.052857      0.402792    0.866653\n",
      "2    best_rf  mser_simulated -1.305678      0.367470    0.748768\n",
      "3   best_etr  mser_simulated  0.175763      0.238281    0.601358\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'mser_physical'\n",
    "dataset_name = 'mser_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_mser(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_stitch(dataset_path, dataset_name,dataset_name_n,path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "    dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                             , encoder_isa = None, encoder_mem_type=None)\n",
    "    encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                                   , encoder_isa = None, encoder_mem_type=None)\n",
    "    \n",
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4',\n",
    "                                                        'isa_1','isa_2' ,'isa_3', 'isa_4', 'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns, total_data_n.columns, len(total_data.columns), len(total_data_n.columns))\n",
    "    total_data = total_data.fillna(0)\n",
    "    total_data_n = total_data_n.fillna(0)\n",
    " \n",
    "    X_sim = total_data.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_sim = total_data[['runtime', 'power']].to_numpy()\n",
    "    X_phy = total_data_n.drop(columns = ['runtime', 'power']).to_numpy()\n",
    "    Y_phy = total_data_n[['runtime','power']].to_numpy()    \n",
    "    print(X_sim.shape, X_phy.shape, Y_sim.shape, Y_phy.shape)\n",
    "\n",
    "    # Separating Physical data to 10% and 90%\n",
    "    X_train_phy, X_test_phy, Y_train_phy, Y_test_phy = train_test_split(X_phy, Y_phy, test_size = 0.90, random_state = 0)\n",
    "    print(X_train_phy.shape, X_test_phy.shape, Y_train_phy.shape, Y_test_phy.shape)\n",
    "    X_train_sim = np.append(X_sim, X_train_phy,axis = 0)\n",
    "    Y_train_sim = np.append(Y_sim, Y_train_phy,axis = 0)\n",
    "    print(X_train_sim.shape, Y_train_sim.shape, X_test_phy.shape, Y_test_phy.shape)\n",
    "    \n",
    "    X_train = X_train_sim\n",
    "    X_test = X_test_phy\n",
    "    Y_train = Y_train_sim\n",
    "    Y_test = Y_test_phy\n",
    "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    \n",
    "    scaler_X_sim = StandardScaler()\n",
    "    scaler_X_phy = StandardScaler()\n",
    "    scaler_X_sim.fit(X_sim)\n",
    "    scaler_X_phy.fit(X_phy)\n",
    "    \n",
    "    scaler_Y_sim = StandardScaler()\n",
    "    scaler_Y_phy = StandardScaler()\n",
    "    scaler_Y_sim.fit(Y_sim)\n",
    "    scaler_Y_phy.fit(Y_phy)\n",
    "    \n",
    "    X_train = scaler_X_sim.transform(X_train)\n",
    "    X_test = scaler_X_phy.transform(X_test)\n",
    "    # Y_train = np.reshape(Y_train, (len(Y_train),1))\n",
    "    # Y_test = np.reshape(Y_test, (len(Y_test),1))\n",
    "    Y_train = scaler_Y_sim.transform(Y_train)\n",
    "    Y_test = scaler_Y_phy.fit_transform(Y_test)    \n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    '''pca = PCA(n_components=9)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)'''\n",
    "\n",
    "    # pca = PCA(n_components=9)\n",
    "    # pca.fit(X_test)\n",
    "    # X_test = pca.transform(X_test)\n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    \n",
    "    # 4. KNN\n",
    "    param_grid_knn =   {'n_neighbors': [ 6, 7, 13, 15],  \n",
    "             'weights' : ['uniform', 'distance'],\n",
    "              'p' : [1, 2, 4, 5, 7 ,10]\n",
    "             } \n",
    "    model_knn = KNeighborsRegressor()          \n",
    "    # best_knn = return_best_param(model_knn, param_grid_knn, X_train, Y_train) \n",
    "    \n",
    "    model_dt = DecisionTreeRegressor()          \n",
    "    # best_dt = return_best_param(model_dt, param_grid_dt, X_train, Y_train) \n",
    "\n",
    "    # 7. Random Forest \n",
    "    param_grid_rf =   {'n_estimators' : [50,  200],  \n",
    "              'max_depth': [5,9,15,20]\n",
    "\n",
    "             } \n",
    "    model_rf = RandomForestRegressor()          \n",
    "    # best_rf = return_best_param(model_rf, param_grid_rf, X_train, Y_train) \n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    param_grid_etr =   {'n_estimators' : [50, 200],\n",
    "              'max_depth': [5,9,15,20]\n",
    "                       }\n",
    "    model_etr = ExtraTreesRegressor()          \n",
    "    # best_etr =  return_best_param(model_etr, param_grid_etr, X_train, Y_train) \n",
    "    \n",
    "    \n",
    "    # return_best_param(model_xgb, param_grid_xgb, X_train, Y_train)\n",
    "    \n",
    "    # best_models = [best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr]\n",
    "    best_models = [model_knn, model_dt, model_rf, model_etr]\n",
    "    best_models_name = [ 'best_knn', 'best_dt', 'best_rf', 'best_etr']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mape_runtime', 'mape_power'])\n",
    "    \n",
    "    for model in best_models:\n",
    "        model_orig = model\n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mape_scores_runtime = []\n",
    "        mape_scores_power = []\n",
    "\n",
    "        fold = 1\n",
    "        print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        model_orig.fit(X_train, Y_train)\n",
    "        Y_pred_fold = model_orig.predict(X_test)\n",
    "        Y_test_fold = scaler_Y_phy.inverse_transform(Y_test)\n",
    "        Y_pred_fold = scaler_Y_phy.inverse_transform(Y_pred_fold)\n",
    "\n",
    "        \n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mape_scores_runtime.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_scores_power.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores[0], 'mape_runtime': mape_scores_runtime[0],'mape_power': mape_scores_power[0]}\n",
    "                       , ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('result_multivariate_stitch' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'runtime', 'power'],\n",
      "      dtype='object') 18 18\n",
      "(425, 16) (52, 16) (425, 2) (52, 2)\n",
      "(5, 16) (47, 16) (5, 2) (47, 2)\n",
      "(430, 16) (430, 2) (47, 16) (47, 2)\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 1 with Model Name:  best_knn\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 2 with Model Name:  best_dt\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 3 with Model Name:  best_rf\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "Running model number: 4 with Model Name:  best_etr\n",
      "(430, 16) (47, 16) (430, 2) (47, 2)\n",
      "  model_name      dataset_name        r2  mape_runtime  mape_power\n",
      "0   best_knn  stitch_simulated -0.055239      0.162916    0.590057\n",
      "1    best_dt  stitch_simulated -1.492504      0.204408    0.789479\n",
      "2    best_rf  stitch_simulated -0.367194      0.186141    0.541177\n",
      "3   best_etr  stitch_simulated  0.038316      0.152759    0.531736\n"
     ]
    }
   ],
   "source": [
    "dataset_name_n = 'stitch_physical'\n",
    "dataset_name = 'stitch_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "path_for_saving_data = dataset_name\n",
    "process_all_stitch(dataset_path, dataset_name, dataset_name_n, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSTkvfRcTArg"
   },
   "source": [
    "# Dataset Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_n = 'matmul_physical'\n",
    "dataset_name = 'matmul_simulated'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_2_Aditya\\\\Dataset\\\\'\n",
    "\n",
    "df = pd.read_csv(dataset_path + dataset_name + '.csv')\n",
    "dfn = pd.read_csv(dataset_path + dataset_name_n + '.csv')\n",
    "encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df\n",
    "                                                                         , encoder_isa = None, encoder_mem_type=None)\n",
    "encoded_data_frame_n, encoder_isa_n, encoder_mem_type_n = encode_text_features('encode', dfn\n",
    "                                                                               , encoder_isa = None, encoder_mem_type=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 519)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_data_frame), len(encoded_data_frame_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['arch', 'cpu-clock', 'isa_1', 'l1d_assoc', 'l1d_cache_lines',\n",
       "        'l1d_shared_by_threads', 'l1d_size', 'l2_assoc', 'l2_cache_lines',\n",
       "        'l2_shared_by_threads', 'l2_size', 'l3_assoc', 'l3_cache_lines',\n",
       "        'l3_shared_by_threads', 'l3_size', 'mem-size', 'mem-type_1',\n",
       "        'mem-type_2', 'mem_clock', 'num-cpus', 'PS', 'num-cpu', 'bus_speed',\n",
       "        'runtime', 'power'],\n",
       "       dtype='object'), 25)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_frame_n.columns, len(encoded_data_frame_n.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['arch', 'cpu-clock', 'isa_1', 'isa_2', 'l1d_assoc', 'l1d_cache_lines',\n",
       "        'l1d_shared_by_threads', 'l1d_size', 'l2_assoc', 'l2_cache_lines',\n",
       "        'l2_shared_by_threads', 'l2_size', 'l3_assoc', 'l3_cache_lines',\n",
       "        'l3_shared_by_threads', 'l3_size', 'mem-size', 'mem-type_1',\n",
       "        'mem-type_2', 'mem-type_3', 'mem-type_4', 'mem_clock', 'num-cpus', 'PS',\n",
       "        'runtime', 'power'],\n",
       "       dtype='object'), 26)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_frame.columns, len(encoded_data_frame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') \n",
      " Index(['cpu-clock', 'l1d_assoc', 'l1d_cache_lines', 'l1d_shared_by_threads',\n",
      "       'l1d_size', 'l2_assoc', 'l2_cache_lines', 'l2_shared_by_threads',\n",
      "       'l2_size', 'l3_assoc', 'l3_cache_lines', 'l3_shared_by_threads',\n",
      "       'l3_size', 'mem-size', 'mem_clock', 'num-cpus', 'PS', 'runtime',\n",
      "       'power'],\n",
      "      dtype='object') 19 19\n"
     ]
    }
   ],
   "source": [
    "    total_data_n = encoded_data_frame_n.drop(columns = ['arch','mem-type_1','mem-type_2','isa_1',\n",
    "                                                        'bus_speed', 'num-cpu'])\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch','mem-type_1','mem-type_2','mem-type_3','mem-type_4','isa_1',\n",
    "                                                    'isa_2'])\n",
    "    print(total_data.columns,'\\n', total_data_n.columns, len(total_data.columns), len(total_data_n.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "All_Models_Sim_to_PHY.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
