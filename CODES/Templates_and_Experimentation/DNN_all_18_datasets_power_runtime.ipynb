{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_features(encode_decode, data_frame, encoder_isa=None, encoder_mem_type=None):\n",
    "    # Implement Categorical OneHot encoding for ISA and mem-type\n",
    "    if encode_decode == 'encode':\n",
    "        encoder_isa = ce.one_hot.OneHotEncoder(cols=['isa'])\n",
    "        encoder_mem_type = ce.one_hot.OneHotEncoder(cols=['mem-type'])\n",
    "        encoder_isa.fit(data_frame, verbose=1)\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        encoder_mem_type.fit(df_new1, verbose=1)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "    else:\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "        \n",
    "    return encoded_data_frame, encoder_isa, encoder_mem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_percentage_error(Y_test, Y_pred):\n",
    "    error = 0\n",
    "    for i in range(len(Y_test)):\n",
    "        if(Y_test[i]!= 0 ):\n",
    "            error = error + (abs(Y_test[i] - Y_pred[i]))/Y_test[i]\n",
    "        \n",
    "    error = error/ len(Y_test)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    # define the keras model\n",
    "    # DNN 1\n",
    "    dnn = Sequential()\n",
    "    dnn.add(Dense(64, input_dim = input_dim, activation='relu'))\n",
    "    # dnn.add(Dropout(0.1))\n",
    "    dnn.add(Dense(64, activation='relu'))\n",
    "    # dnn.add(Dropout(0.1))\n",
    "    dnn.add(Dense(64, activation='relu'))\n",
    "    # dnn.add(Dropout(0.1))\n",
    "    dnn.add(Dense(2, activation='linear'))\n",
    "    # print('Model : DNN 2', dnn.summary())\n",
    "    # plot_model(dnn, to_file = 'dnn.png', show_shapes = True)\n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X = total_data.drop(columns = ['runtime','power']).to_numpy()\n",
    "    # X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data[['runtime', 'power']].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "    ################## Data Preprocessing ######################\n",
    "\n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape_runtime','mape_power', 'mae' ])\n",
    "    print('####################################################################')\n",
    "    r2_scores = []\n",
    "    mse_scores = []\n",
    "    mape_runtime_scores = []\n",
    "    mape_power_scores = []\n",
    "    mae_scores = []\n",
    "\n",
    "    cv = ShuffleSplit(n_splits = 10, random_state = 0, test_size = 0.2)\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        model_orig = create_model(input_dim = X.shape[1])\n",
    "\n",
    "        X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "        scaler_x = StandardScaler()\n",
    "        X_train_fold = scaler_x.fit_transform(X_train_fold)\n",
    "        # X_train = scaler_x.fit_transform(X_train)\n",
    "        X_test_fold = scaler_x.fit_transform(X_test_fold)\n",
    "        scaler_y = StandardScaler()\n",
    "        # Y = scaler_y.fit_transform(Y)\n",
    "        Y_train_fold = scaler_y.fit_transform(Y_train_fold)\n",
    "        Y_test_fold = scaler_y.fit_transform(Y_test_fold)\n",
    "        model_orig.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "        model_orig.fit(X_train_fold, Y_train_fold, epochs = 100, batch_size = 5, verbose = 0)\n",
    "        Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "\n",
    "        # save the folds to disk\n",
    "        data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "        filename = path_for_saving_data + '/folds_data/' +'dnn' +'_'+ str(fold) + '.pickle'\n",
    "        pickle.dump(data, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "        # save the model to disk\n",
    "        # serialize model to JSON\n",
    "        filename_1 = path_for_saving_data + '/models_data/' + 'dnn' + '_' + str(fold) + '.json'\n",
    "        filename_2 = path_for_saving_data + '/models_data/' + 'dnn' + '_' + str(fold) + '.h5'\n",
    "        fold = fold + 1\n",
    "        model_json = model_orig.to_json()\n",
    "        with open(filename_1, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model_orig.save_weights(filename_2)\n",
    "        print(\"Saved model to disk for cross validation step :\", fold-1)\n",
    "\n",
    "        # later...\n",
    "        '''\n",
    "        # load json and create model\n",
    "        json_file = open('model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(\"model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        '''\n",
    "        # some time later...\n",
    "        '''\n",
    "        # load the model from disk\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        result = loaded_model.score(X_test, Y_test)\n",
    "        print(result)\n",
    "        '''\n",
    "        # scores.append(best_svr.score(X_test, y_test))\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.plot(Y_test_fold, 'b')\n",
    "        plt.plot(Y_pred_fold, 'r')\n",
    "        '''\n",
    "        # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "        Y_pred_fold = scaler_y.inverse_transform(Y_pred_fold)\n",
    "        Y_test_fold = scaler_y.inverse_transform(Y_test_fold)\n",
    "        r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "        mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "        mape_runtime_scores.append(absolute_percentage_error(Y_test_fold[:,0], Y_pred_fold[:,0]))\n",
    "        mape_power_scores.append(absolute_percentage_error(Y_test_fold[:,1], Y_pred_fold[:,1]))\n",
    "        mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "\n",
    "    df = df.append({'model_name': 'dnn', 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape_runtime': mape_runtime_scores,'mape_power': mape_power_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "    k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv('\\\\DNN_results\\\\' + dataset_name + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'matmul_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'matmul_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: dijkstra_simulated\n",
      "Data X and Y shape (362, 22) (362, 2)\n",
      "Train Test Split: (289, 22) (73, 22) (289, 2) (73, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name        dataset_name  \\\n",
      "0        dnn  dijkstra_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9751907413307365, 0.9883722336340972, 0.973...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [5538537.74640093, 10006822.53119802, 16539826...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.04295668956292717, 0.056536714402518166, 0....   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.23113102935042557, 0.09615743780011285, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [1024.9732631267984, 1487.01212118017, 1866.01...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'dijkstra_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: svm_simulated\n",
      "Data X and Y shape (390, 22) (390, 2)\n",
      "Train Test Split: (312, 22) (78, 22) (312, 2) (78, 2)\n",
      "####################################################################\n",
      "WARNING:tensorflow:From C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name   dataset_name  \\\n",
      "0        dnn  svm_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9904417855186105, 0.989701078778101, 0.9871...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [1370431.7260963307, 2018206.479364916, 773414...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.019697018586891775, 0.019491081813543146, 0...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.1360996667832664, 0.09870003853505545, 0.16...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [606.6889333402701, 685.0264175583078, 493.493...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'svm_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: svm_physical\n",
      "Data X and Y shape (52, 26) (52, 2)\n",
      "Train Test Split: (41, 26) (11, 26) (41, 2) (11, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name  dataset_name                                                 r2  \\\n",
      "0        dnn  svm_physical  [0.4720475509611476, 0.8579091271970247, 0.187...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [5415236.214816459, 1722881.4519262502, 429994...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.09248844557349489, 0.04838956059135608, 0.0...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.11376805361690283, 0.11705269564355435, 0.3...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [1578.3497519217833, 916.2017522617045, 1190.7...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'svm_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: tracking_physical\n",
      "Data X and Y shape (52, 26) (52, 2)\n",
      "Train Test Split: (41, 26) (11, 26) (41, 2) (11, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name       dataset_name  \\\n",
      "0        dnn  tracking_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.786876737096216, 0.7556139958185053, 0.7130...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [620189.5826894549, 1859593.686386227, 2211780...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.07877138100803924, 0.09443734404922847, 0.1...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.1013845092595577, 0.1598243315989578, 0.085...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [522.0012876953605, 689.5979257289681, 819.625...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tracking_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: tracking_simulated\n",
      "Data X and Y shape (425, 22) (425, 2)\n",
      "Train Test Split: (340, 22) (85, 22) (340, 2) (85, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name        dataset_name  \\\n",
      "0        dnn  tracking_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9909958254606885, 0.9865979223988, 0.980709...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [68890.39565567755, 321425.16922584985, 393436...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.011796401829065134, 0.024233399206642497, 0...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.14836990645769604, 0.1276887514738028, 0.19...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [137.98159139473734, 284.3151137081137, 303.28...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tracking_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: stitch_physical\n",
      "Data X and Y shape (52, 26) (52, 2)\n",
      "Train Test Split: (41, 26) (11, 26) (41, 2) (11, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name     dataset_name  \\\n",
      "0        dnn  stitch_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.7100145407956665, 0.9152769781240822, 0.811...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [2890886.376534916, 962445.2868391402, 1558760...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.07158177163690409, 0.057205511189421826, 0....   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.09394973781737269, 0.11585940685470529, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [831.7415041981294, 592.9055543123579, 834.235...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'stitch_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: stitch_simulated\n",
      "Data X and Y shape (425, 22) (425, 2)\n",
      "Train Test Split: (340, 22) (85, 22) (340, 2) (85, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name      dataset_name  \\\n",
      "0        dnn  stitch_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9923456179166921, 0.984609171546237, 0.9917...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [276794.8452023225, 259301.07284741732, 592022...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.013733831280799457, 0.012095431812804112, 0...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.09663205250369415, 0.14907801375718566, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [263.0340081062684, 254.45398083129956, 388.58...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'stitch_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: mser_physical\n",
      "Data X and Y shape (52, 26) (52, 2)\n",
      "Train Test Split: (41, 26) (11, 26) (41, 2) (11, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name   dataset_name  \\\n",
      "0        dnn  mser_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.7629031285410384, 0.7275281193130377, 0.740...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [130181.9864344397, 368161.81668669026, 416694...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.08386662272145418, 0.1270961486897805, 0.14...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.15553946816267586, 0.11663558751765933, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [216.03149322561794, 330.71990516391816, 346.3...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'mser_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: mser_simulated\n",
      "Data X and Y shape (430, 22) (430, 2)\n",
      "Train Test Split: (344, 22) (86, 22) (344, 2) (86, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name    dataset_name  \\\n",
      "0        dnn  mser_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9947005062750076, 0.9796324165524922, 0.985...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [14520.628286588488, 83645.40846077373, 91207....   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.015263370307931566, 0.03227517732161418, 0....   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.11079192468513031, 0.06952487807090417, 0.0...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [65.68908377454639, 121.23292282137498, 186.59...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'mser_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: montecarlocalcpi_physical\n",
      "Data X and Y shape (260, 22) (260, 2)\n",
      "Train Test Split: (208, 22) (52, 22) (208, 2) (52, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name               dataset_name  \\\n",
      "0        dnn  montecarlocalcpi_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9234896656953197, 0.9025932743322768, 0.913...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [319819281584.06854, 3844514294799.753, 131928...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.642320663716837, 0.5565887464853838, 0.6589...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.14587444562908863, 0.12359810097900029, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [334919.18199282134, 936432.266053452, 514653....  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'montecarlocalcpi_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: montecarlocalcpi_simulated\n",
      "Data X and Y shape (1365, 23) (1365, 2)\n",
      "Train Test Split: (1092, 23) (273, 23) (1092, 2) (273, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name                dataset_name  \\\n",
      "0        dnn  montecarlocalcpi_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9958960287706026, 0.9940884983718232, 0.994...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [1696842.5258429872, 1536755.8560916628, 12909...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.14186017947983065, 0.25344910509973656, 0.0...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.3497361317192296, 0.18880027928912446, 0.22...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [741.8745182250876, 818.6821485919096, 475.186...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'montecarlocalcpi_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: qsort_physical\n",
      "Data X and Y shape (672, 22) (672, 2)\n",
      "Train Test Split: (537, 22) (135, 22) (537, 2) (135, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name    dataset_name  \\\n",
      "0        dnn  qsort_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.8955967412352415, 0.9288838511515438, 0.870...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [1594720.9698034774, 438937.28761789633, 19910...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.21587111836038858, 0.1500765467852115, 0.22...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.13024185544860026, 0.20647802984151306, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [491.4999135467668, 288.26148489317393, 580.06...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'qsort_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: qsort_simulated\n",
      "Data X and Y shape (2730, 23) (2730, 2)\n",
      "Train Test Split: (2184, 23) (546, 23) (2184, 2) (546, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name     dataset_name  \\\n",
      "0        dnn  qsort_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.998134145269316, 0.9973872554165604, 0.9969...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [2236.942092665856, 2296.295906163151, 1344.68...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.021716952722580597, 0.02828055077724673, 0....   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.05531454677639483, 0.0662926845274661, 0.14...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [20.037826864933937, 26.3031600288596, 18.5004...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'qsort_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: sha_simulated\n",
      "Data X and Y shape (367, 22) (367, 2)\n",
      "Train Test Split: (293, 22) (74, 22) (293, 2) (74, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name   dataset_name  \\\n",
      "0        dnn  sha_simulated   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.9681358500612449, 0.9918126562924627, 0.983...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [859872.2632164496, 288059.6605304792, 1066618...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.02808264740724807, 0.022017423055326956, 0....   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.09061923582088735, 0.0982726257746076, 0.10...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [430.8306607855785, 305.2890231954023, 586.438...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sha_simulated'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model for Dataset: sha_physical\n",
      "Data X and Y shape (52, 21) (52, 2)\n",
      "Train Test Split: (41, 21) (11, 21) (41, 2) (11, 2)\n",
      "####################################################################\n",
      "Saved model to disk for cross validation step : 1\n",
      "Saved model to disk for cross validation step : 2\n",
      "Saved model to disk for cross validation step : 3\n",
      "Saved model to disk for cross validation step : 4\n",
      "Saved model to disk for cross validation step : 5\n",
      "Saved model to disk for cross validation step : 6\n",
      "Saved model to disk for cross validation step : 7\n",
      "Saved model to disk for cross validation step : 8\n",
      "Saved model to disk for cross validation step : 9\n",
      "Saved model to disk for cross validation step : 10\n",
      "  model_name  dataset_name                                                 r2  \\\n",
      "0        dnn  sha_physical  [0.8841573722968434, 0.8036197675155613, 0.800...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [2343448.255951007, 6583634.1713729, 5961103.3...   \n",
      "\n",
      "                                        mape_runtime  \\\n",
      "0  [0.05179688149991248, 0.11691870503766583, 0.1...   \n",
      "\n",
      "                                          mape_power  \\\n",
      "0  [0.0700802507023195, 0.08864831272922208, 0.06...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [654.3386392733696, 1571.239899277205, 1501.03...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sha_physical'\n",
    "print('Running Model for Dataset:', dataset_name)\n",
    "dataset_path = '\\\\ALL_CSV\\\\Dataset\\\\' + dataset_name + '.csv'\n",
    "path_for_saving_data = '\\\\Saved_Models_Data\\\\' + dataset_name\n",
    "process_all(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
